{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17bdbc19-0866-495a-8441-a5ec82619055",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from langchain_core.tools import tool\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True, dotenv_path=\".env\")\n",
    "\n",
    "# Retrieve the directory from environment variables\n",
    "LOG_DIR = os.getenv(\"LOG_DIRECTORY\", \"./logs\")\n",
    "\n",
    "@tool\n",
    "def list_log_files() -> List[str]:\n",
    "    \"\"\"Lists all .log files in the configured log directory.\"\"\"\n",
    "    try:\n",
    "        path = Path(LOG_DIR)\n",
    "        if not path.is_dir():\n",
    "            return [f\"Error: {LOG_DIR} is not a valid directory.\"]\n",
    "        \n",
    "        return [f.name for f in path.glob(\"*.log\")]\n",
    "    except Exception as e:\n",
    "        return [f\"Error listing logs: {str(e)}\"]\n",
    "\n",
    "@tool\n",
    "def read_log_file(filename: str, last_n_lines: int = 20) -> str:\n",
    "    \"\"\"\n",
    "    Reads the last N lines of a specific log file from the log directory.\n",
    "    Only the filename (e.g., 'server.log') is required.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Join the base LOG_DIR with the filename for security\n",
    "        path = Path(LOG_DIR) / filename\n",
    "        \n",
    "        if not path.exists():\n",
    "            return f\"Error: File {filename} not found in {LOG_DIR}\"\n",
    "\n",
    "        with open(path, 'r', encoding='utf-8', errors='replace') as f:\n",
    "            return \"\".join(deque(f, maxlen=last_n_lines))\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"Error reading log: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41ce97cd-873c-496b-9876-b1f627993aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import ToolNode\n",
    "# from log_reader import read_log_file, list_log_files\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables - MUST come before ChatOpenAI()\n",
    "load_dotenv(override=True, dotenv_path=\".env\")\n",
    "\n",
    "# Validate API key\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\n",
    "        \"OPENAI_API_KEY not found. Please set it in your .env file or environment variables.\"\n",
    "    )\n",
    "\n",
    "# Get model name from environment or use default\n",
    "model_name = os.getenv(\"OPENAI_MODEL\", \"gpt-5-mini\")\n",
    "\n",
    "# Register all available tools\n",
    "tools = [read_log_file, list_log_files]\n",
    "\n",
    "# Create model with tools\n",
    "model = ChatOpenAI(model=model_name).bind_tools(tools)\n",
    "tool_node = ToolNode(tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4b97e76-a687-44a7-9e18-4d3285659e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Node: agent ---\n",
      "Calling Tools...\n",
      "--- Node: tools ---\n",
      "[\"server.log\"]\n",
      "--- Node: agent ---\n",
      "Calling Tools...\n",
      "--- Node: tools ---\n",
      "93.184.216.34 - - [31/Jan/2026:15:00:00 +0000] \"GET /index.html HTTP/1.1\" 200 1045 \"-\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\"\n",
      "192.168.1.10 - specific_user [31/Jan/2026:15:05:30 +0000] \"POST /api/login HTTP/1.1\" 401 120 \"https://www.example.com/login\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15\"\n",
      "10.0.0.5 - - [31/Jan/2026:15:10:15 +0000] \"GET /images/logo.png HTTP/1.1\" 304 0 \"https://www.example.com/\" \"Googlebot/2.1 (+http://www.google.com)\" \n",
      "\n",
      "--- Node: agent ---\n",
      "I inspected the log directory and the only log file present is server.log. I read the most recent entries from that file; they are standard HTTP access entries (GET/POST) and show no server crashes or error stack traces. Summary:\n",
      "\n",
      "- Files found: server.log\n",
      "- server.log (recent lines): normal access entries (200, 401, 304). No 5xx responses, no \"ERROR\", \"Exception\", \"panic\", \"segfault\" or stack traces present in the excerpt.\n",
      "- Conclusion: No recurring crashes are visible in the provided log file.\n",
      "\n",
      "If you want to continue hunting for crashes, here are recommended next steps and useful commands you can run locally to search the logs and system for recurring crashes or errors:\n",
      "\n",
      "1) Search all logs in that directory for common error/crash keywords:\n",
      "- grep -inE \"error|exception|panic|segfault|fatal|traceback|crash\" /path/to/logs/*.log\n",
      "\n",
      "Example:\n",
      "- grep -inE \"error|exception|panic|segfault|fatal|traceback|crash\" /Users/rradhakrishnan/git/interview_prep/langchain/agentic_ai/log_analyzer/*.log\n",
      "\n",
      "2) If your application writes separate error or application logs, check those (common names: error.log, app.log, stderr.log, out.log). List files first:\n",
      "- ls -la /Users/rradhakrishnan/git/interview_prep/langchain/agentic_ai/log_analyzer/\n",
      "\n",
      "3) If your app runs as a systemd service:\n",
      "- sudo journalctl -u your-service-name --since \"7 days ago\" | grep -iE \"error|exception|segfault|panic\"\n",
      "\n",
      "4) For containerized apps:\n",
      "- docker ps\n",
      "- docker logs <container> --since \"24h\" | grep -iE \"error|exception|panic\"\n",
      "\n",
      "5) Kernel / OS level crashes:\n",
      "- dmesg | grep -iE \"segfault|panic|oops\"\n",
      "- /var/log/syslog or /var/log/messages (platform dependent)\n",
      "\n",
      "6) If you want to detect recurring crashes over time, aggregate and count errors:\n",
      "- grep -i \"ExceptionName\" *.log | awk '{print $0}' | sort | uniq -c | sort -rn\n",
      "- Or use a log tool (ELK, Loki, Splunk) and create alerts.\n",
      "\n",
      "7) If you want me to search further, either:\n",
      "- give me permission to read additional files in that directory, or\n",
      "- paste relevant log files / excerpts that contain errors.\n",
      "\n",
      "If you want, I can run targeted searches (examples above) on any other files you allow me to read, or help set up monitoring/alerting for future recurring crashes.\n",
      "--- Node: summarize ---\n",
      "Log Analysis Summary\n",
      "\n",
      "1) Root Cause\n",
      "- No crash was found in the provided logs. The server.log excerpt contains only normal HTTP access entries (status codes 200, 401, 304) and no error strings, stack traces, or 5xx responses.\n",
      "- Most likely explanations:\n",
      "  - No application/server crash occurred during the inspected period, or\n",
      "  - Crash events are logged to a different file/location (error/app logs, system journal, rotated logs, container logs) or were lost due to rotation/retention settings, or\n",
      "  - The application suppresses or redirects crash diagnostics (no core dump, limited logging level).\n",
      "\n",
      "2) Timestamp\n",
      "- Inspected log entries (from server.log):\n",
      "  - 31/Jan/2026:15:00:00 +0000 ‚Äî GET /index.html ‚Äî 200\n",
      "  - 31/Jan/2026:15:05:30 +0000 ‚Äî POST /api/login ‚Äî 401\n",
      "  - 31/Jan/2026:15:10:15 +0000 ‚Äî GET /images/logo.png ‚Äî 304\n",
      "- Conclusion: No crash timestamp available in the provided logs. The above are the most recent timestamps observed.\n",
      "\n",
      "3) Suggested Fix (prioritized, actionable)\n",
      "- Immediate verification steps\n",
      "  1. Search all logs in the directory for error/crash keywords:\n",
      "     - grep -inE \"error|exception|panic|segfault|fatal|traceback|crash\" /Users/rradhakrishnan/git/interview_prep/langchain/agentic_ai/log_analyzer/*.log\n",
      "  2. List the directory to confirm there are no other log files:\n",
      "     - ls -la /Users/rradhakrishnan/git/interview_prep/langchain/agentic_ai/log_analyzer/\n",
      "  3. If the app runs as a system service, inspect system logs:\n",
      "     - sudo journalctl -u <service-name> --since \"7 days ago\" | grep -iE \"error|exception|panic\"\n",
      "  4. For containerized deployments:\n",
      "     - docker ps\n",
      "     - docker logs <container> --since \"24h\" | grep -iE \"error|exception|panic\"\n",
      "\n",
      "- If crashes are suspected but not found\n",
      "  5. Enable or increase application logging (ERROR/DEBUG) and ensure errors are written to a persistent error log (error.log or app.log).\n",
      "  6. Enable core dumps or crash reporting for the runtime so you can capture native crashes:\n",
      "     - ulimit -c unlimited (and configure /proc/sys/kernel/core_pattern)\n",
      "  7. Check rotated/archived logs (logrotate) and the log retention configuration; inspect older files (e.g., server.log.1, .gz).\n",
      "\n",
      "- Long-term / monitoring\n",
      "  8. Add automated monitoring/alerting for 5xx responses, uncaught exceptions and process restarts (Prometheus + Alertmanager, Sentry, ELK/Loki).\n",
      "  9. Centralize logs (ELK, Loki, Splunk) to allow quick searching and set alerts for recurring error patterns.\n",
      "  10. Implement health checks and process supervisors (systemd, supervisord, k8s liveness probes) to detect and restart on failure and to log the restart reason.\n",
      "\n",
      "If you want, I can:\n",
      "- Run the grep search on any additional files you permit me to read, or\n",
      "- Re-scan if you provide other log files (error/app logs, rotated logs, or system journal excerpts).\n",
      "üì¶ Using project: log-analyzer\n",
      "‚úÖ Found existing project: log-analyzer\n",
      "‚úÖ Found existing dataset: log-analyzer\n",
      "‚úÖ Uploaded 1 logs to dataset: log-analyzer\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Annotated, Sequence, TypedDict, Literal\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langsmith import traceable\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "# from openai_model import model, tool_node\n",
    "\n",
    "# Optional import for LangSmith upload functionality\n",
    "try:\n",
    "    from upload_to_langsmith import upload_logs_to_langsmith\n",
    "except ImportError:\n",
    "    upload_logs_to_langsmith = None\n",
    "\n",
    "# 1. Configuration & State\n",
    "load_dotenv(override=True)\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "# 2. Nodes\n",
    "@traceable(run_type=\"llm\")\n",
    "def call_model(state: AgentState):\n",
    "    \"\"\"The 'Brain' - decides which logs to read.\"\"\"\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def summarize_results(state: AgentState):\n",
    "    \"\"\"The 'Synthesizer' - converts raw log data into a clean report.\"\"\"\n",
    "    summary_prompt = HumanMessage(content=(\n",
    "        \"You have finished reading the logs. Provide a structured 'Log Analysis Summary' \"\n",
    "        \"including: 1. Root Cause, 2. Timestamp, and 3. Suggested Fix.\"\n",
    "    ))\n",
    "    # We pass the full history to the model for the final summary\n",
    "    response = model.invoke(state[\"messages\"] + [summary_prompt])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# 3. Routing Logic (The Conditional Edge)\n",
    "@traceable\n",
    "def router(state: AgentState) -> Literal[\"tools\", \"summarize\", \"agent\"]:\n",
    "    last_msg = state[\"messages\"][-1]\n",
    "    \n",
    "    # Path A: Model wants to use a tool (list_dir, read_file)\n",
    "    if getattr(last_msg, \"tool_calls\", None):\n",
    "        return \"tools\"\n",
    "    \n",
    "    # Path B: Detect if we found a crash but might need more context (Stack Trace)\n",
    "    # If 'Traceback' is in the text, we let the agent loop once more to get detail\n",
    "    if \"Traceback\" in last_msg.content and len(state[\"messages\"]) < 6:\n",
    "        return \"agent\"\n",
    "    \n",
    "    # Path C: Information gathered, move to summary\n",
    "    return \"summarize\"\n",
    "\n",
    "# 4. Graph Construction\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "workflow.add_node(\"summarize\", summarize_results)\n",
    "\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_conditional_edges(\"agent\", router)\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "workflow.add_edge(\"summarize\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# 5. Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    log_dir = os.getenv(\"LOG_DIRECTORY\", \"./logs\")\n",
    "    \n",
    "    inputs = {\n",
    "        \"messages\": [\n",
    "            (\"user\", f\"Analyze the logs in '{log_dir}'. Identify any recurring crashes.\")\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for output in app.stream(inputs, stream_mode=\"updates\"):\n",
    "        for node, data in output.items():\n",
    "            print(f\"--- Node: {node} ---\")\n",
    "            print(data[\"messages\"][-1].content or \"Calling Tools...\")\n",
    "    \n",
    "    # Optional: Upload logs to LangSmith if function is available\n",
    "    if upload_logs_to_langsmith:\n",
    "        upload_logs_to_langsmith(\"log-analyzer\", log_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1a9a4a",
   "metadata": {},
   "source": [
    "Evaluate Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3146ccd5-be38-4d52-b56d-097b7fe94732",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LangSmith SDK Evaluation Script\n",
    "\n",
    "This script runs programmatic evaluations of the log analyzer agent using LangSmith SDK.\n",
    "It tests the agent against a dataset of realistic queries and evaluates the responses.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langsmith import Client, traceable\n",
    "from langsmith.evaluation import evaluate\n",
    "from langsmith.schemas import Run\n",
    "\n",
    "from main import app\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True, dotenv_path=\".env\")\n",
    "\n",
    "# Initialize LangSmith client\n",
    "client = Client(api_key=os.getenv(\"LANGSMITH_API_KEY\"))\n",
    "PROJECT_NAME = os.getenv(\"LANGSMITH_PROJECT\", \"log-analyzer\")\n",
    "\n",
    "\n",
    "def load_evaluation_dataset() -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load evaluation dataset from JSON file.\"\"\"\n",
    "    dataset_path = Path(__file__).parent / \"evaluation_dataset.json\"\n",
    "    with open(dataset_path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "@traceable(name=\"log_analyzer_agent\")\n",
    "def run_agent(query: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run the agent with a given query and return the final response.\n",
    "    This is wrapped with @traceable for LangSmith tracking.\n",
    "    \"\"\"\n",
    "    from langchain_core.messages import HumanMessage\n",
    "    \n",
    "    log_dir = os.getenv(\"LOG_DIRECTORY\", \"./logs\")\n",
    "    \n",
    "    inputs = {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=query)\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Collect all messages from the stream\n",
    "    final_messages = []\n",
    "    for output in app.stream(inputs, stream_mode=\"updates\"):\n",
    "        for node, data in output.items():\n",
    "            if \"messages\" in data and data[\"messages\"]:\n",
    "                final_messages.extend(data[\"messages\"])\n",
    "    \n",
    "    # Get the last message content (final response)\n",
    "    if final_messages:\n",
    "        last_message = final_messages[-1]\n",
    "        if hasattr(last_message, \"content\"):\n",
    "            return {\"output\": last_message.content}\n",
    "        else:\n",
    "            return {\"output\": str(last_message)}\n",
    "    \n",
    "    return {\"output\": \"No response generated\"}\n",
    "\n",
    "\n",
    "def agent_predict(inputs: Dict[str, str]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Wrapper function for the agent that LangSmith can evaluate.\n",
    "    This is the function that gets called during evaluation.\n",
    "    \"\"\"\n",
    "    query = inputs.get(\"query\", \"\")\n",
    "    result = run_agent(query)\n",
    "    return {\"output\": result.get(\"output\", \"\")}\n",
    "\n",
    "\n",
    "def contains_evaluator(run: Run, example) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Custom evaluator that checks if the output contains expected keywords.\n",
    "    \"\"\"\n",
    "    prediction = run.outputs.get(\"output\", \"\").lower()\n",
    "    expected = example.outputs.get(\"expected_contains\", [])\n",
    "    \n",
    "    if not expected:\n",
    "        return {\"key\": \"contains_check\", \"score\": 1.0}\n",
    "    \n",
    "    found_count = sum(1 for keyword in expected if keyword.lower() in prediction)\n",
    "    score = found_count / len(expected) if expected else 0.0\n",
    "    \n",
    "    return {\n",
    "        \"key\": \"contains_check\",\n",
    "        \"score\": score,\n",
    "        \"comment\": f\"Found {found_count}/{len(expected)} expected keywords\"\n",
    "    }\n",
    "\n",
    "\n",
    "def structure_evaluator(run: Run, example) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Custom evaluator that checks if the output has expected structure elements.\n",
    "    \"\"\"\n",
    "    prediction = run.outputs.get(\"output\", \"\").lower()\n",
    "    expected_structure = example.outputs.get(\"expected_structure\", [])\n",
    "    \n",
    "    if not expected_structure:\n",
    "        return {\"key\": \"structure_check\", \"score\": 1.0}\n",
    "    \n",
    "    found_count = sum(1 for element in expected_structure if element.lower() in prediction)\n",
    "    score = found_count / len(expected_structure) if expected_structure else 0.0\n",
    "    \n",
    "    return {\n",
    "        \"key\": \"structure_check\",\n",
    "        \"score\": score,\n",
    "        \"comment\": f\"Found {found_count}/{len(expected_structure)} expected structure elements\"\n",
    "    }\n",
    "\n",
    "\n",
    "def min_score_evaluator(run: Run, example) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluator that checks if the overall score meets the minimum threshold.\n",
    "    \"\"\"\n",
    "    min_score = example.outputs.get(\"min_score\", 0.7)\n",
    "    \n",
    "    # Calculate average of other evaluators\n",
    "    contains_score = contains_evaluator(run, example).get(\"score\", 0.0)\n",
    "    structure_score = structure_evaluator(run, example).get(\"score\", 0.0)\n",
    "    avg_score = (contains_score + structure_score) / 2\n",
    "    \n",
    "    passed = avg_score >= min_score\n",
    "    \n",
    "    return {\n",
    "        \"key\": \"min_score_check\",\n",
    "        \"score\": 1.0 if passed else 0.0,\n",
    "        \"comment\": f\"Average score {avg_score:.2f} {'meets' if passed else 'below'} minimum {min_score}\"\n",
    "    }\n",
    "\n",
    "\n",
    "def run_evaluation(project_name: str = None, project_url: str = None):\n",
    "    \"\"\"\n",
    "    Run the evaluation experiment using LangSmith SDK.\n",
    "    \n",
    "    Args:\n",
    "        project_name: LangSmith project name. If not provided, uses LANGSMITH_PROJECT env var.\n",
    "        project_url: LangSmith project URL. If not provided, constructs from project_name.\n",
    "    \"\"\"\n",
    "    if project_name is None:\n",
    "        project_name = os.getenv(\"LANGSMITH_PROJECT\", \"log-analyzer\")\n",
    "    \n",
    "    if project_url is None:\n",
    "        project_url = f\"https://smith.langchain.com/projects/{project_name}\"\n",
    "    \n",
    "    print(f\"üöÄ Starting LangSmith evaluation for project: {project_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load dataset\n",
    "    dataset = load_evaluation_dataset()\n",
    "    print(f\"üìä Loaded {len(dataset)} test cases\")\n",
    "    \n",
    "    # Create or get dataset in LangSmith\n",
    "    dataset_name = f\"{project_name}-dataset\"\n",
    "    try:\n",
    "        # Try to get existing dataset\n",
    "        client.read_dataset(dataset_name=dataset_name)\n",
    "        print(f\"üìÅ Using existing dataset: {dataset_name}\")\n",
    "    except Exception:\n",
    "        # Create new dataset\n",
    "        try:\n",
    "            client.create_dataset(\n",
    "                dataset_name=dataset_name,\n",
    "                description=\"Log analyzer agent evaluation dataset\"\n",
    "            )\n",
    "            print(f\"üìÅ Created new dataset: {dataset_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Dataset creation issue (may already exist): {e}\")\n",
    "    \n",
    "    # Upload examples to dataset (use raw dicts; LangSmith assigns ids)\n",
    "    try:\n",
    "        client.create_examples(\n",
    "            inputs=[item[\"inputs\"] for item in dataset],\n",
    "            outputs=[item[\"outputs\"] for item in dataset],\n",
    "            dataset_name=dataset_name\n",
    "        )\n",
    "        print(f\"‚úÖ Uploaded {len(dataset)} examples to dataset\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Example upload issue (may already exist): {e}\")\n",
    "        print(f\"   Continuing with existing examples...\")\n",
    "    \n",
    "    # Run evaluation\n",
    "    print(\"\\nüîç Running evaluation...\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    results = evaluate(\n",
    "        agent_predict,\n",
    "        data=dataset_name,\n",
    "        evaluators=[\n",
    "            contains_evaluator,\n",
    "            structure_evaluator,\n",
    "            min_score_evaluator,\n",
    "        ],\n",
    "        experiment_prefix=f\"{project_name}-experiment\",\n",
    "        max_concurrency=1,  # Run sequentially to avoid rate limits\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìà Evaluation Results Summary\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Wait for experiment feedback to be processed so we can read results\n",
    "    results.wait()\n",
    "    \n",
    "    # Aggregate scores by evaluator key\n",
    "    scores_by_key: Dict[str, List[float]] = {}\n",
    "    n_results = 0\n",
    "    for row in results:\n",
    "        n_results += 1\n",
    "        eval_results = getattr(row, \"evaluation_results\", None)\n",
    "        if eval_results is None:\n",
    "            continue\n",
    "        res_list = getattr(eval_results, \"results\", [])\n",
    "        for r in res_list or []:\n",
    "            key = getattr(r, \"key\", \"unknown\")\n",
    "            score = getattr(r, \"score\", None)\n",
    "            if score is not None and isinstance(score, (int, float)):\n",
    "                scores_by_key.setdefault(key, []).append(float(score))\n",
    "    \n",
    "    if not scores_by_key and n_results == 0:\n",
    "        print(\"\\n‚ö†Ô∏è  No evaluation results found. Results may still be processing.\")\n",
    "        print(\"   Check the LangSmith UI in a minute for full feedback.\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ Evaluation completed ({n_results} runs)\")\n",
    "        if scores_by_key:\n",
    "            print(\"\\n  Evaluator scores (average):\")\n",
    "            for key in sorted(scores_by_key.keys()):\n",
    "                vals = scores_by_key[key]\n",
    "                avg = sum(vals) / len(vals) if vals else 0\n",
    "                print(f\"    {key}: {avg:.2f}  (n={len(vals)})\")\n",
    "        print(f\"\\nüìä View full results in LangSmith:\")\n",
    "        print(f\"   {project_url}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Validate API key\n",
    "    if not os.getenv(\"LANGSMITH_API_KEY\"):\n",
    "        raise ValueError(\n",
    "            \"LANGSMITH_API_KEY not found. Please set it in your .env file.\"\n",
    "        )\n",
    "    \n",
    "    run_evaluation()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
